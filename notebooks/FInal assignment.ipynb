{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Final assignment.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoBettini/Stock-Market-Prediction-2020/blob/main/notebooks/FInal%20assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsGE7MtNbNVs"
      },
      "source": [
        "# Take-home Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPNvlfFxcz__"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CWouFAF6uni"
      },
      "source": [
        "# To plot figures\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# To make this notebook's output stable across runs\n",
        "random_state = 42\n",
        "np.random.seed(random_state)\n",
        "tf.random.set_seed(random_state)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (15.0, 10.0)\n",
        "plt.rcParams['figure.facecolor'] = 'white'\n",
        "plt.rcParams['figure.subplot.bottom'] = 0.125\n",
        "plt.rcParams['figure.edgecolor'] = 'white'\n",
        "plt.rcParams[\"savefig.dpi\"] = 300\n",
        "\n",
        "plt.rcParams['font.size'] = 15\n",
        "plt.rcParams['axes.labelsize'] = 15\n",
        "plt.rcParams['xtick.labelsize'] = 15\n",
        "plt.rcParams['ytick.labelsize'] = 15"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "CEJNWXun-qIC",
        "outputId": "7760cd65-dae4-4df8-cf96-c58e0b642334"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "'''\n",
        "ax = nasdaq_df.plot(y=[\"Close\"])\n",
        "\n",
        "fig = ax.get_figure()\n",
        "fig.savefig('/content/drive/My Drive/Stock_market_figures/close.png')\n",
        "'''"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \"\"\"\n\u001b[0;32m--> 566\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-141bd059c49a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m '''\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnasdaq_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Close\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1psKSMlbaTq"
      },
      "source": [
        "# Dataset exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRHqSUOcbia9"
      },
      "source": [
        "In this section we will upload and explore the dataset \"**Processed_NASDAQ**\",  containing several daily features of NASDAQ Composite from 2010 to 2017. The dataset was acquired from [this repository](https://archive.ics.uci.edu/ml/datasets/CNNpred%3A+CNN-based+stock+market+prediction+using+a+diverse+set+of+variables#).\n",
        "\n",
        "It covers features from various categories of technical indicators, future contracts, price of commodities, important indices of markets around the world, price of major companies in the U.S. market, and treasury bill rates. Sources and thorough description of features have been mentioned in the paper \"[CNNpred: CNN-based stock market prediction using a diverse set of variables](https://arxiv.org/pdf/1810.08923.pdf)\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCsh7PUUhJGr"
      },
      "source": [
        "## 1 - Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUkpwKrvhiSw"
      },
      "source": [
        "nasdaq_url = 'https://raw.githubusercontent.com/MatteoBettini/Stock-Market-Prediction-2020/main/stock_markets_datasets/Processed_NASDAQ.csv?token=ANHXQQOLNU7CE7V2OTZJ43C76A552'\n",
        "dji_url = 'https://raw.githubusercontent.com/MatteoBettini/Stock-Market-Prediction-2020/main/stock_markets_datasets/Processed_DJI.csv?token=ANHXQQOXYQZSSSTVFKX6RZS74XDXA'\n",
        "nyse_url = 'https://raw.githubusercontent.com/MatteoBettini/Stock-Market-Prediction-2020/main/stock_markets_datasets/Processed_NYSE.csv?token=ANHXQQNAISMPCLVLRTGNJBC74XD2C'\n",
        "russel_url = 'https://raw.githubusercontent.com/MatteoBettini/Stock-Market-Prediction-2020/main/stock_markets_datasets/Processed_RUSSELL.csv?token=ANHXQQPGLBLSM3B36OLWIPC74XD3U'\n",
        "s_p_url = 'https://raw.githubusercontent.com/MatteoBettini/Stock-Market-Prediction-2020/main/stock_markets_datasets/Processed_S%26P.csv?token=ANHXQQNRFS3NKP2XCF5Q5MS74XD5K'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pZ70QaBhO4R"
      },
      "source": [
        "nasdaq_df = pd.read_csv(nasdaq_url, parse_dates=[\"Date\"], index_col=\"Date\")\n",
        "# Dataset is now stored in a Pandas Dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh7ghtuiqJM7"
      },
      "source": [
        "We load the .csv file telling pandas to parse the date column and to use it to index the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR7jQ9Oh8baW"
      },
      "source": [
        "## 2 - Exploring the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCsIO_GuhWth"
      },
      "source": [
        "Now that we have loaded the dataset we can start inspecting the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSFutgMZiUBF"
      },
      "source": [
        "nasdaq_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4GxW5-18oc2"
      },
      "source": [
        "Taking a peak at the first ten elements we can already see that there are a lot of missing values. They will be treated accordingly in [this section](#missing_values).\n",
        "\n",
        "\n",
        "We can also see that the dates inculuded in the dataset are referring only to working days as the stock market is open only on those days.\n",
        "\n",
        "We can get a confirmation of this by looking at the following rows where we see that 16,17 January 2010 are not present because it was a weekend and 18 January 2010 is not present because of the federal U.S. festivity of \"Martin Luther King Jr. Day\".\n",
        "\n",
        "This is not a problem for our machine learning pipeline as we will map the 'Date' feature into a categorical feature representing the day of the week.\n",
        "More information on this will be provided in [this section](#cathegorical_values)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21RoCezJ_BFP"
      },
      "source": [
        "nasdaq_df.iloc[9:13]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihUKgC2EQtZp"
      },
      "source": [
        "### Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_heXnGwcQza9"
      },
      "source": [
        "The datasets described in [the paper](https://arxiv.org/pdf/1810.08923.pdf) contain 1984 entries, each representing a day of trading in a stock market. Each entry has 82 features which are grouped in the following way:\n",
        "\n",
        "*   Primitive features\n",
        "*   Technical indicators\n",
        "*   Economic data\n",
        "*   World stock markets\n",
        "*   The exchange rate of U.S. dollar\n",
        "*   Commodities\n",
        "*   Big U.S. Companies\n",
        "*   Futures contracts\n",
        "\n",
        "The authors have made available five datasets, each representing a different stock market. The available markets are: S&P 500, NASDAQ Composite, Dow Jones Industrial Average, RUSSELL 2000, and NYSE Composite. In this work we will explore and analyse the NASDAQ Composite dataset, but all the insights we are going to gain will be valid for all datasets.\n",
        "\n",
        "The primitive featrues and the technical indicators are unique for each dataset, while all the other features are common among datasets.\n",
        "\n",
        "A tabular description of the features is also reported in the following images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC669folRApS"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/MatteoBettini/Stock-Market-Prediction-2020/main/feature_description/feature_table_1.png?token=ANHXQQI7BINXDEPRGLZ42LS74XAH6\" width=\"2000\">\n",
        "<img src=\"https://raw.githubusercontent.com/MatteoBettini/Stock-Market-Prediction-2020/main/feature_description/feature_table_2.png?token=ANHXQQOG2ZP4BPHXIWXNWWK74XAKY\" width=\"2000\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTXAAWzyUUDv"
      },
      "source": [
        "Let's reorder the features in our data frame to match the description."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adWxtZr2PeuO"
      },
      "source": [
        "technical_indeces = [\"Volume\",\"mom\",\"mom1\",\"mom2\",\"mom3\",\"ROC_5\",\"ROC_10\",\"ROC_15\",\"ROC_20\",\"EMA_10\",\"EMA_20\",\"EMA_50\",\"EMA_200\"]\n",
        "economic_indices = [\"DTB4WK\",\"DTB3\",\"DTB6\",\"DGS5\",\"DGS10\",\"DAAA\",\"DBAA\",\"TE1\",\"TE2\",\"TE3\",\"TE5\",\"TE6\",\"DE1\",\"DE2\",\"DE4\",\"DE5\",\"DE6\",\"CTB3M\",\"CTB6M\",\"CTB1Y\"]\n",
        "comodities_indices = [\"Oil\",\"Brent\",\"WIT-oil\",\"Gold\",\"gold-F\",\"XAU\",\"XAG\",\"GAS-F\",\"silver-F\",\"copper-F\",\"wheat-F\"]\n",
        "word_indices = [\"GSPC\",\"DJI\",\"NYSE\",\"RUT\",\"HSI\",\"SSEC\",\"FCHI\",\"FTSE\",\"GDAXI\"]\n",
        "exchannge_indices = [\"JPY\",\"GBP\",\"CAD\",\"CNY\",\"AUD\",\"NZD\",\"CHF\",\"EUR\",\"Dollar index\",\"Dollar index-F\"]\n",
        "companies_indices = [\"XOM\",\"JPM\",\"AAPL\",\"MSFT\",\"GE\",\"JNJ\",\"WFC\",\"AMZN\"]\n",
        "futures_indices = [\"CAC-F\",\"FTSE-F\",\"DAX-F\",\"HSI-F\",\"Nikkei-F\",\"KOSPI-F\",\"NASDAQ-F\",\"DJI-F\",\"S&P-F\",\"RUSSELL-F\"]\n",
        "\n",
        "\n",
        "nasdaq_df = nasdaq_df[[\"Name\",\"Close\"] +\n",
        "                       technical_indeces +\n",
        "                       economic_indices +\n",
        "                       comodities_indices +\n",
        "                       word_indices +\n",
        "                       exchannge_indices +\n",
        "                       companies_indices +\n",
        "                       futures_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugy7NaCG_znP"
      },
      "source": [
        "By looking at the info of the NASDAQ dataset we can see features, their types and the number of non-null values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX5fSSOH7l9I"
      },
      "source": [
        "nasdaq_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E_1IVe0AY7G"
      },
      "source": [
        "If we analyze carefully the features availaible in the actual dataset we may notice a few differences from those described in the paper.\n",
        "\n",
        "Apart from the fact that the same feature may have different names in the two descriptions, in the loaded dataset we may find two **new** features:\n",
        "\n",
        "| # | Feature | Description | Type |\n",
        "| --- | --- | --- | --- |\n",
        "| 83 | mom | Return of 1 day before | Technical indicator |\n",
        "| 84| wheat-F | Relative change of wheat price| Comodity |\n",
        "\n",
        "The features descriptions and types come from my best intution, as I could not find any detailed description ot these two features.\n",
        "\n",
        "In each market's dataset we may also see that there is a feature indicating the name of the market from where the data comes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNgpMhTc7l6g"
      },
      "source": [
        "nasdaq_df[\"Name\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIUC65QkUn-G"
      },
      "source": [
        "For now we are considering only one market so we can drop this feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O396AZ1VVKLH"
      },
      "source": [
        "nasdaq_df.drop(labels=[\"Name\"], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT78Rn6QQ7MJ"
      },
      "source": [
        "<a name=\"return_equation_cell\"/>\n",
        "\n",
        "Lastly, **mom** represents the return of 1 day before at time $t$. So among the features IXIC, GSPC, DJI, NYSE and RUSSEL, the one referring to the market from where the dataset is from is dropped as the same data is contained in **mom**.\n",
        " \n",
        "The return of a market (mom) at time $t$ can be computed from the \"Close\" feature as $$Return_{t} = \\frac{Close_{t}}{Close_{t-1}} - 1$$\n",
        "\n",
        "For example, in our dataset *Processed_NASDAQ* the feature IXIC is not present.\n",
        "\n",
        "Therefore, we can understand why in our data frame we have 84 columns: Date (which is the index) and 83 features. This is the result of the addition of \"mom\", \"wheat-F\" and \"Name\" to the 82 features described in the paper considering the absence of one feature among IXIC, GSPC, DJI, NYSE and RUSSEL.\n",
        "\n",
        "All features are floats except for Date (that has been parsed as datetime64) and Name that is a string.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQegjPTTt5nJ"
      },
      "source": [
        "We also want to insert the feature describing the day of the week.\n",
        "\n",
        "We do this in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkJHCcdBupFh"
      },
      "source": [
        "nasdaq_df.insert(0,'day_of_week',nasdaq_df.index.dayofweek)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrHOpP4TvGqK"
      },
      "source": [
        "nasdaq_df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "573Yn64fvAy4"
      },
      "source": [
        "nasdaq_df[\"day_of_week\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7OeW1c67l3f"
      },
      "source": [
        "nasdaq_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDKrkvdGSd7W"
      },
      "source": [
        "This gives us some information on how the data are distributed. NaN values are excluded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8yyP1CPTrXg"
      },
      "source": [
        "## 3 - Plotting the time series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMsBYL-jT2Sz"
      },
      "source": [
        "The data in this dataset are representing a **time series**.\n",
        "\n",
        "This is not the typical dataset for a supervised learning prediction, so we will have to modify some things in order to apply the supervised learning algorithms we know. This will be done in [this section](#transforming_the_dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azSVtQaWUVbS"
      },
      "source": [
        "But first, let's see the evolution over time of the close price of the NASDAQ market."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbbIyqQuT2EX"
      },
      "source": [
        "ax = nasdaq_df.plot(y=[\"Close\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqccj9OEwP_L"
      },
      "source": [
        "We can also plot the returns for all the markets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-7C50_J7ls1"
      },
      "source": [
        "nasdaq_df.plot(y=[\"mom\",\"NYSE\",\"GSPC\",\"DJI\",\"RUT\"], subplots=True, figsize=(20,15))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KscmjuzxlLx"
      },
      "source": [
        "We can see that there are some common trends.\n",
        "\n",
        "One example is the big oscillation we can observe between 2011 and 2012\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxQvy_GG7COb"
      },
      "source": [
        "Another interesting aspect to look into is the comparison of Nasdaq return among the different years.\n",
        "\n",
        "As for different years we may have a different number of data points, here we compare only the years that have the same amount of data points and for which the amount corresponds to the maximum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3DVnzld7lqS"
      },
      "source": [
        "groups = nasdaq_df[\"mom\"].groupby(pd.Grouper(freq='A'))\n",
        "\n",
        "max_size = 0\n",
        "for _, group in groups:\n",
        "    if group.values.shape[0] > max_size:\n",
        "        max_size = group.values.shape[0]\n",
        "\n",
        "years = pd.DataFrame()\n",
        "for name, group in groups:\n",
        "    if group.values.shape[0] == max_size:\n",
        "        years[name.year] = group.values\n",
        "\n",
        "years.plot(subplots=True, xlabel=\"Day of the year\", title=\"Nasdaq return (mom) compared among different years\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZYnvNDS7a0j"
      },
      "source": [
        "Apart from some common trends between 2013 and 2014, there are no major similarities between different years.\n",
        "\n",
        "This gives us an intuition that return depends more on other features rather than time of the year.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSxdu5Y-_Tsr"
      },
      "source": [
        "Here we report also a whisker plot for the return of NASDAQ over the years.\n",
        "\n",
        "This plot draws a box around the 25th and 75th percentiles of the data that captures the middle 50% of observations. A line is drawn at the 50th percentile (the median) and whiskers are drawn above and below the box to summarize the general extents of the observations. Dots are drawn for outliers outside the whiskers or extents of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUtJRs4H-1vV"
      },
      "source": [
        "years.boxplot(rot=35)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojAKmrRh8v6V"
      },
      "source": [
        "Another interesting plot to visualize is the distribution of the featrues, we can look at this by plotting an histogram for each feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP5ipEi99FX4"
      },
      "source": [
        "#nasdaq_df.hist(figsize=(30,35), bins=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK5bc9iHqnE2"
      },
      "source": [
        "We see that a great part of the features is distributed as a gaussian centered in 0.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqiizs-JaAX8"
      },
      "source": [
        "## 4 - Splitting the data: training, validation and test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feqlva-0amCL"
      },
      "source": [
        "As already mentioned, our dataset represents a time series. Therefore, random or stratified train/test split strategies cannot be used as they would not enable the machine learning algorithm to benefit from the fact that the data are temporally correlated with each other.\n",
        "\n",
        "Given this considerations, the only possibility we have is to split the dataset in the most traditional way:\n",
        "\n",
        "*   The first 60% of the entries will form the **training set**\n",
        "*   The subsequent 20% will form the **validation set**\n",
        "*   The last 20% will form the **test set**\n",
        "\n",
        "Before performing the split we also compute the target vector, for each entry, the target is 1 if there is an increase in the Close price in the following entry and 0 otherwise.\n",
        "\n",
        "$$ Target_{t} = \\begin{cases} 1, & \\mbox{if } Close_{t+1}>Close_{t} \\\\ 0, & \\mbox{else} \\end{cases}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOjhxEYmwMSf"
      },
      "source": [
        "target_df =  nasdaq_df[\"Close\"].shift(-1) - nasdaq_df[\"Close\"]\n",
        "target = np.where(target_df.values > 0, 1, 0)\n",
        "target = target[:-1] # We have to remove the last value as we have no target for it\n",
        "\n",
        "target.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QsrjIxYcyHT"
      },
      "source": [
        "nasdaq_df = nasdaq_df.iloc[:-1,:] # We have to remove the last value as we have no target for it\n",
        "\n",
        "nasdaq_train = nasdaq_df[:int(nasdaq_df.shape[0]*0.6)]\n",
        "nasdaq_train_target = target[:int(target.shape[0]*0.6)]\n",
        "\n",
        "nasdaq_val = nasdaq_df[int(nasdaq_df.shape[0]*0.6):int(nasdaq_df.shape[0]*0.8)]\n",
        "nasdaq_val_target = target[int(target.shape[0]*0.6):int(target.shape[0]*0.8)]\n",
        "\n",
        "nasdaq_test =  nasdaq_df[int(nasdaq_df.shape[0]*0.8):]\n",
        "nasdaq_test_target = target[int(target.shape[0]*0.8):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTnDe0THezUs"
      },
      "source": [
        "print(nasdaq_train.shape, nasdaq_train_target.shape,\n",
        "      nasdaq_val.shape, nasdaq_val_target.shape,\n",
        "      nasdaq_test.shape, nasdaq_test_target.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vLU7FzyfR_Z"
      },
      "source": [
        "The sets are still in the form of a time series. We will procede to make it suitable for a supervised learning classical task in [this section](#transforming_the_dataset)\n",
        "\n",
        "Given the nature of the data it is very difficult to split the dataset properly. This is because the data in the time series is closely correlated with each other and with time. Certain features refer to timesteps that go back even 200 days in the data (like EMA_200).\n",
        "\n",
        "Therefore it is impossible to ensure that the training, validation and test set will be independent between each other. Our evaluations will always be a bit biased. Note that this is caused by the fact that the data we are analysing is a multivariate time series and by the fact that some features refer to previous timesteps.\n",
        "\n",
        "To ensure the least interdependence between the sets we split them in the shown way. Only after this split we will proceed to build the superviesed learning dataset. Therefore for each set the first $n-1$ targets (where $n$ is the number of training days considered for the prediction) will not be used, as those are the targets that come from input that is partially of other sets or is not available.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6_UzuefQwtr"
      },
      "source": [
        "Furthermore, we can look at the proportion of the target values in each set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YebpS7oQz57"
      },
      "source": [
        "def subset_proportions(subset):\n",
        "    props = {}\n",
        "    for value in set(subset):\n",
        "        data_value = [i for i in subset if i==value]\n",
        "        props[value] = len(data_value) / len(subset)\n",
        "    return props\n",
        "\n",
        "   \n",
        "compare_props = pd.DataFrame({\n",
        "    \"Overall\": subset_proportions(target),\n",
        "    \"Train\": subset_proportions(nasdaq_train_target),\n",
        "    \"Validation\": subset_proportions(nasdaq_val_target),\n",
        "    \"Test\" : subset_proportions(nasdaq_test_target),\n",
        "})\n",
        "\n",
        "compare_props"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFuoFPttRAL2"
      },
      "source": [
        "We can see that they are pretty balanced. Each class is well represented.\n",
        "\n",
        "All the three sets of data mantain the proportions of each class similar to the original proportions in the full dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP5BlLYJ6iDy"
      },
      "source": [
        "## 5 - Correlations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F96kp3wZf4jl"
      },
      "source": [
        "Now that we have split the dataset we can use our training and validation sets to gain some insights on the correlation of the features.\n",
        "\n",
        "Let's use nasdaq_train_val for this objective."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCVFwR5PgGHY"
      },
      "source": [
        "nasdaq_train_val = nasdaq_df[:int(nasdaq_df.shape[0]*0.8)]\n",
        "nasdaq_train_val_target = target[:int(target.shape[0]*0.8)]\n",
        "\n",
        "nasdaq_train_val.shape, nasdaq_train_val_target.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmHmxSi-gcEV"
      },
      "source": [
        "By doing this we ensure no data leakage between the training/validation sets and test set. The results on the test set are thus a fair evaluation the algorithm's performance.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAJzSIYBo-ee"
      },
      "source": [
        "### Autocorrelation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWM4QVxNxAU_"
      },
      "source": [
        "As we said our dataset contains a time series.\n",
        "\n",
        "Therefore we can analyse the time series of various features.\n",
        "\n",
        "Let's build the series for:\n",
        "\n",
        "*   Close, the close price of the Nasdaq market\n",
        "*   mom, Nasdaq return"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oDP9FVMxxle"
      },
      "source": [
        "close_series = pd.Series(nasdaq_train_val[\"Close\"])\n",
        "\n",
        "return_series = pd.Series(nasdaq_train_val[\"mom\"].bfill()) # We temporarly backward fill the only NaN in the mom column as we need all the values for plotting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzIfUkMPyIhm"
      },
      "source": [
        "Now that we have mapped the two series to the pandas Series object we can look at their autocorrelation plots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK5tcSR95g54"
      },
      "source": [
        "The autocorrelation plot is a way of measuring and explaining the internal association between observations in a time series. We can check how strong an internal correlation is in an given amount of time.\n",
        "\n",
        "The autocorrelation plot shows how much the value of the time series is correlated with itself $n$ timesteps in the past. $n$ is called lag.\n",
        "\n",
        "The dotted lines are just above and below the first quartile, or within the 95% confidence interval. This will indicate the significance of the correlation. If the line is above or below the dotted line, not in between, we can say that the correlation is significant, and that the close value is correlated to time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_ekaZsL7lnu"
      },
      "source": [
        "from pandas.plotting import autocorrelation_plot\n",
        "\n",
        "autocorrelation_plot(close_series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbahl5w25kzf"
      },
      "source": [
        "In this plot we can see that the close price is highly correlated with itself in the first part of the graph. The correlation decreases with the increase of the lag as we can expect.\n",
        "This is intuitive as the close price is a variable that we expect to be correlated with itself for small lags.\n",
        "\n",
        "There is an interesting negative correlation in the second half of the plot. This could suggest a trend of stock markets to rebalance themselves every three-four years. This could also be an effect of the econimic crisis.\n",
        "\n",
        "This suggestes that there is an association between time and close price."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EKJOiIx7llB"
      },
      "source": [
        "autocorrelation_plot(return_series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz5zvGP81KtT"
      },
      "source": [
        "This plot shows that the return of NASDAQ is not correlated with itself in the past and therefore we will rely on other features to predict our target.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bscD189eh39u"
      },
      "source": [
        "Now let's look at the correlation among the features.\n",
        "\n",
        "First we will plot the correlation matrix among features of the same category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I2xUYfSXFwc"
      },
      "source": [
        "primitive_indices = [\"Close\",\"mom\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPLVY-GXXLFF"
      },
      "source": [
        "These are the features that resemble our target, therefore we will see the correlation of these with all the other features grouped by cathegory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoOkpwWc4c4J"
      },
      "source": [
        "### Technical indicators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZtpHNfTiIaJ"
      },
      "source": [
        "Let's see the correlation between primitive features and technincal indicators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q8Jucx17liI"
      },
      "source": [
        "technical_indeces.remove(\"mom\")\n",
        "correlation_matrix = nasdaq_train_val[primitive_indices + technical_indeces].corr()\n",
        "correlation_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qnV6A627ldR"
      },
      "source": [
        "correlation_matrix[\"mom\"].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiDWIX5K7laF"
      },
      "source": [
        "correlation_matrix[\"Close\"].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS4f4MSB7lXu"
      },
      "source": [
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "attributes = [\"Close\", \"mom\",\"mom1\", \"ROC_5\", \"ROC_10\", \"EMA_10\",\"EMA_20\"]\n",
        "scatter_matrix(nasdaq_train_val[attributes])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2URXtUnHSkOw"
      },
      "source": [
        "We see that the close price is very correlated with all the EMA features and the return closely correlates with the ROC features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fptzF7eoTzkP"
      },
      "source": [
        "### Economic indicators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MayAgQa7lUu"
      },
      "source": [
        "correlation_matrix = nasdaq_train_val[primitive_indices + economic_indices].corr()\n",
        "correlation_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy21xy9d7lSO"
      },
      "source": [
        "correlation_matrix[\"mom\"].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5emLuHL17lPZ"
      },
      "source": [
        "correlation_matrix[\"Close\"].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4rPVwo47lMh"
      },
      "source": [
        "attributes = [\"Close\", \"mom\", \"TE6\", \"DE4\", \"DBAA\"]\n",
        "scatter_matrix(nasdaq_train_val[attributes])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z3Nvl4sY3w8"
      },
      "source": [
        "We can see many otther significant correlations in the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H464QcrZB5b"
      },
      "source": [
        "### Word indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZzP2udu7lJt"
      },
      "source": [
        "correlation_matrix = nasdaq_train_val[primitive_indices + word_indices].corr()\n",
        "correlation_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU5vYZVc7lG0"
      },
      "source": [
        "correlation_matrix[\"mom\"].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtM9oKB37lBG"
      },
      "source": [
        "correlation_matrix[\"Close\"].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRkYuiqimXCc"
      },
      "source": [
        "attributes = primitive_indices + word_indices[:4]\n",
        "scatter_matrix(nasdaq_train_val[attributes])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbeelEu5iTck"
      },
      "source": [
        "Here we can se that all the returns from the stock markets are very closely correlated with each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bToN60Yam-mJ"
      },
      "source": [
        "### All features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoW4P_I8nLs7"
      },
      "source": [
        "Now we look at the correlation between the primitive features and all other features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV3AHrCXn_Mu"
      },
      "source": [
        "correlation_matrix = nasdaq_train_val.corr()\n",
        "correlation_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9ynd8MkoFKb"
      },
      "source": [
        "correlation_matrix[\"mom\"].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNnRQ9L3oKwI"
      },
      "source": [
        "correlation_matrix[\"Close\"].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjstXoM5n-4k"
      },
      "source": [
        "As we can see the return is very correlated with the returns of other stock markets and with its and their futures. We can also see a significant negative correlation with the relative change in the U.S dollar index.\n",
        "\n",
        "The close price is very correlated with the exponential moving average and negatively correlated with economic features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7gScURSou6e"
      },
      "source": [
        "## 6 - Data transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNEPrZpzqPLp"
      },
      "source": [
        "Now that we have explored the attributes and their correlation we can transform our data to prepare it for the machine learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9SurgKlq_Jh"
      },
      "source": [
        "<a name=\"missing_values\"></a>\n",
        "\n",
        "### Missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEYysWuUrCoP"
      },
      "source": [
        "In our dataset there are some missing values. Therefore we need to define a strategy to treat them.\n",
        "\n",
        "Given the fact that we are working with time series we use the interpolation function provided by pandas, which interpolates the missing values using the time index.\n",
        "\n",
        "Then to fill the remaining missing values in the first rows of the dataset, we perform backwards filling using pandas' `bfill()` function. This function propagates backwards the first valid value to fill the remaining missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSbGVq87rZ-M"
      },
      "source": [
        "nasdaq_train = nasdaq_train.interpolate(method='time')\n",
        "nasdaq_train = nasdaq_train.bfill()\n",
        "\n",
        "nasdaq_train_val = nasdaq_train_val.interpolate(method='time')\n",
        "nasdaq_train_val = nasdaq_train_val.bfill()\n",
        "\n",
        "nasdaq_val = nasdaq_val.interpolate(method='time')\n",
        "nasdaq_val = nasdaq_val.bfill()\n",
        "\n",
        "nasdaq_test = nasdaq_test.interpolate(method='time')\n",
        "nasdaq_test = nasdaq_test.bfill()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUQT84XWG5Ts"
      },
      "source": [
        "<a name=\"cathegorical_values\"></a>\n",
        "\n",
        "### Categorical features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIo0efplG8s8"
      },
      "source": [
        "In the dataset we have just one categorical feature which is \"day_of_week\"\n",
        "\n",
        "It is already expressed as an int, with the following mapping:\n",
        "\n",
        "*   0 -> Monday\n",
        "*   1 -> Tuesday\n",
        "*   2 -> Wednesday\n",
        "*   3 -> Thursday\n",
        "*   4 -> Friday\n",
        "\n",
        "In this case we do not need to map it to a one-hot encoding, as we want the alogrithm to benefit from the similarity given from this encoding of the days of the week.\n",
        "\n",
        "Note that we never have data on Saturdays and Sundays, as in those days the stock market is closed.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv3vpj5lIHC3"
      },
      "source": [
        "### Feature scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmWgemRHIQqd"
      },
      "source": [
        "Finally, we want to standardise our features as they have various different ranges.\n",
        "\n",
        "To do this we perform standard scaling of the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcfeaIxPK2qs"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "nasdaq_train = pd.DataFrame(data=StandardScaler().fit_transform(nasdaq_train), columns=nasdaq_train.columns, index=nasdaq_train.index)\n",
        "\n",
        "nasdaq_train_val = pd.DataFrame(data=StandardScaler().fit_transform(nasdaq_train_val), columns=nasdaq_train_val.columns, index=nasdaq_train_val.index)\n",
        "\n",
        "nasdaq_val = pd.DataFrame(data=StandardScaler().fit_transform(nasdaq_val), columns=nasdaq_val.columns, index=nasdaq_val.index)\n",
        "\n",
        "nasdaq_test = pd.DataFrame(data=StandardScaler().fit_transform(nasdaq_test), columns=nasdaq_test.columns, index=nasdaq_test.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz9GWeBrGcqq"
      },
      "source": [
        "<a name=\"transforming_the_dataset\"></a>\n",
        "\n",
        "### Dynamic window, transfrming the dataset for supervised learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6eQeyZANZO2"
      },
      "source": [
        "Now that we have prepared our data, we can build the dataset fro our prediction task.\n",
        "\n",
        "To do this we will make use of the following function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34TMKEab6rYy"
      },
      "source": [
        "# from https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n",
        "\n",
        "def series_to_supervised(data, n_in, dropnan=True):\n",
        "    \"\"\"\n",
        "    Frame a time series as a supervised learning dataset.\n",
        "    Arguments:\n",
        "        data: Sequence of observations as a list or NumPy array.\n",
        "        n_in: Number of lag observations as input (n).\n",
        "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
        "    Returns:\n",
        "        Pandas DataFrame of series framed for supervised learning.\n",
        "    \"\"\"\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in - 1, -1, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        if i != 0:\n",
        "            names += [f'{feature}(t-{i})' for feature in nasdaq_df.columns]\n",
        "        else:\n",
        "            names += [f'{feature}(t)' for feature in nasdaq_df.columns]\n",
        "\n",
        "    # put it all together\n",
        "    agg = pd.concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt4IpalXR5ab"
      },
      "source": [
        "This function takes as input the time series dataset and outputs a DataFrame containing for each row $n * 83$ values. These values are the values of the features for all the $n$ previous training days.\n",
        "\n",
        "As mentioned before, for each dataset we drop the first $n-1$ lines as they would have inputs for which we don't have all the data. This is done by setting the *dropnan* flag.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7KnYfSVTKeG"
      },
      "source": [
        "Now we select the number of previous training days that we want to consider for our prediction.\n",
        "\n",
        "With use the value of $60$ as it is the one chosen by the authors of the paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXXnKmDfT6Re"
      },
      "source": [
        "n = 60"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkr_VCp_T1gK"
      },
      "source": [
        "Now let's use the function described above to obtain our new dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwoSNmfh6xn-"
      },
      "source": [
        "nasdaq_train_y = nasdaq_train_target[n-1:]  # We drop the first n-1 targets as we do not have the data to predict them\n",
        "nasdaq_train_X = series_to_supervised(nasdaq_train.values, n)\n",
        "\n",
        "nasdaq_train_X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5D6y-rPUIRT"
      },
      "source": [
        "As we can see now our dataset is suited for the classification task.\n",
        "\n",
        "Let's reshape it into a 3 dimensional array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgRy7cljQABN"
      },
      "source": [
        "nasdaq_train_X = nasdaq_train_X.values\n",
        "nasdaq_train_X_3D = nasdaq_train_X.reshape(-1, n, nasdaq_train.shape[1])\n",
        "\n",
        "nasdaq_train_X_3D.shape, nasdaq_train_y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3AuAF9OUXbn"
      },
      "source": [
        "We obtain a training set with 1130 entries.\n",
        "\n",
        "Each entry has $n * 83$ features, where n represents the number of previous training days considered and 83 is the number of features we have for each day.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHl9zhiME_RV"
      },
      "source": [
        "<a name=\"train_val\"></a>\n",
        "\n",
        "We also convert the set containing both the training and validation set.\n",
        "\n",
        "This set will be bigger than the union of the converted training and validation sets. This is because when we convert it we do not need to throw away the first $n-1$ entries of the validation set.\n",
        "\n",
        "We will use this set for [time series cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_Wpsq7EbIsZ"
      },
      "source": [
        "nasdaq_train_val_y = nasdaq_train_val_target[n-1:]\n",
        "nasdaq_train_val_X = series_to_supervised(nasdaq_train_val.values, n)\n",
        "nasdaq_train_val_X = nasdaq_train_val_X.values\n",
        "nasdaq_train_val_X_3D = nasdaq_train_val_X.reshape(-1, n, nasdaq_train_val.shape[1])\n",
        "\n",
        "nasdaq_train_val_X_3D.shape, nasdaq_train_val_y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF4r96xWE-My"
      },
      "source": [
        "\n",
        "Let's convert also the validation and testing datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W1uGhJ-RIYA"
      },
      "source": [
        "nasdaq_val_y = nasdaq_val_target[n-1:]\n",
        "nasdaq_val_X = series_to_supervised(nasdaq_val.values, n)\n",
        "nasdaq_val_X = nasdaq_val_X.values\n",
        "nasdaq_val_X_3D = nasdaq_val_X.reshape(-1, n, nasdaq_val.shape[1])\n",
        "\n",
        "nasdaq_val_X_3D.shape, nasdaq_val_y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeTUsGozUaX1"
      },
      "source": [
        "nasdaq_test_y = nasdaq_test_target[n-1:]\n",
        "nasdaq_test_X = series_to_supervised(nasdaq_test.values, n)\n",
        "nasdaq_test_X = nasdaq_test_X.values\n",
        "nasdaq_test_X_3D = nasdaq_test_X.reshape(-1, n, nasdaq_test.shape[1])\n",
        "\n",
        "nasdaq_test_X_3D.shape, nasdaq_test_y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzErhDpbU53Z"
      },
      "source": [
        "# Machine learning algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPffYbx_VBsY"
      },
      "source": [
        "Now we will apply some algorithms for supervised learning and, in particular, binary classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltYyb_ljSXGa"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhRUAMutW8s-"
      },
      "source": [
        "We will evaluate our models on the following metrics:\n",
        "\n",
        "\n",
        "* *Accuracy* represents the proportion of correct classifications our model makes over all the samples. In order to maximise accuracy, you'll need to maximise on the number of true positives and true negatives.\n",
        "$$\n",
        "ACC = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "$$\n",
        " \n",
        "* *Precision* measures how reliable or trustworthy your classifier is. It tells you how often when the classifier predicts that the stock market close price will increase in the next day (class $1$) it actually increases. It relies on the number of $TP$'s and $FP$'s:\n",
        "$$\n",
        "P = \\frac{TP}{TP+FP}\n",
        "$$\n",
        "\n",
        "* *Recall* measures the coverage of your classifier. It tells you how many of the actual increases your classifier can detect at all. It relies on the number of $TP$'s and $FN$'s:\n",
        "$$\n",
        "R = \\frac{TP}{TP+FN}\n",
        "$$\n",
        "\n",
        "* Finally, $F_1$*-score* combines the two measures above to give you an overall idea of your classifier's performance. $F_1$*-score* is estimated as follows:\n",
        "$$\n",
        "F_1 = 2 \\times \\frac{P \\times R}{P+R}\n",
        "$$\n",
        "\n",
        "\n",
        "In the paper the authors select Macro-Averaged-$F_{1}$-Measure as their evaluation metric. This is equal to the unweighted mean of the $F_1$ scores between the two classes.\n",
        "\n",
        "Their choice is guided by the fact that accuracy may be biased towards models that tend to predict the more frequent class.\n",
        "\n",
        "We agree with this statement.\n",
        "\n",
        "For this project we will use all the metrics above in their *weighted* version.\n",
        "*Weighted* is  similar to macro, with the difference that the resulting metric is a weighted avarage of the metric for the two classes.\n",
        "\n",
        "In particular the one we are more interested in is the Weighted-$F_1$-Score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZCZrHHnSgns"
      },
      "source": [
        "To see how our model perform, we will also look into:\n",
        "\n",
        "*   the *Confusion matrix*, that represents the number of $TP, TN, FP, FN$ in a matrix:\n",
        "\n",
        "|                  | predicted class 0.  | predicted class 1.   |\n",
        "|  -------------   |   :-------------:   |    :-------------:   |\n",
        "| **actual class 0** | TN                  |   FP                 |\n",
        "| **actual class 1** | FN                  |   TP                 |\n",
        "\n",
        "*  the Receiver Operating Characteristic (ROC) (when possible to plot)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdJGwflWT8Xs"
      },
      "source": [
        "To do this let's import the metrics utilities of sk-learn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw2Wb6DAW7xD"
      },
      "source": [
        "from sklearn.metrics import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOj14O3aoQkV"
      },
      "source": [
        "We also save the *Accuracy* and *Weighted-$F_1$-Score* for each model and for both validation and testing.\n",
        "\n",
        "This will be used at the end to plot the performance of the models and compare them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbGItvB6jtPh"
      },
      "source": [
        "val_performance_acc = {}\n",
        "val_performance_f1_weighted = {}\n",
        "\n",
        "test_performance_acc = {}\n",
        "test_performance_f1_weighted = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNU49peaoHC5"
      },
      "source": [
        "## Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaaGIGWzEEXb"
      },
      "source": [
        "First let's train a logistic regressor.\n",
        "\n",
        "The selected starting learning rate and the learning rate schedule have been chosen after a process of tuning by looking at the results given by the metrics on the validation set.\n",
        "\n",
        "The starting learning rate is **0.4**\n",
        "\n",
        "The learning rate schedule is **adaptive** ([see](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn6YpnGioDVa"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "log_reg = SGDClassifier(max_iter=400, tol=None, random_state=42,\n",
        "                        loss=\"log\", eta0=0.4, learning_rate=\"adaptive\", penalty=None, n_jobs=-1)\n",
        "\n",
        "log_reg.fit(nasdaq_train_X, nasdaq_train_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zher3q12GcTu"
      },
      "source": [
        "Let's see what results it achieved on the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJrjguGOGg-w"
      },
      "source": [
        "pred_train = log_reg.predict(nasdaq_train_X)\n",
        "\n",
        "confusion_matrix(nasdaq_train_y, pred_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxJ_jWF-Gx7U"
      },
      "source": [
        "Cool! It was able to find a separation boundary that correctly classifies all the training instances.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YahOeMvTSCHY"
      },
      "source": [
        "<a name=\"cross_val\"></a>\n",
        "\n",
        "### Cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLelRAJqHPl8"
      },
      "source": [
        "Now let's evaluate how well it generalises performing cross-validation.\n",
        "\n",
        "Given the fact that our input is a time series, we cannot perform stratified K-fold or normal K-fold cross-validation, instead we can just split the time series incrementally (because test indieces must always be higher than training indices [see](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM1HVlvxI_Z4"
      },
      "source": [
        "The default number of splits is 5 and we will keep that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2G_EBb8KSk5"
      },
      "source": [
        "For cross-validation we use the set containing both trianing and validation samples ([see](#train_val))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9pE4PJzGcDe"
      },
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit, cross_validate\n",
        "\n",
        "tscv = TimeSeriesSplit()\n",
        "ts_split = tscv.split(nasdaq_train_val_X, nasdaq_train_val_y)\n",
        "\n",
        "for i, [train_index, test_index] in enumerate(ts_split):\n",
        "    print(f\"GROUP N{i+1}: NUM OF SAMPLES IN THE SETS-> TRAIN: {len(train_index)}, TEST: {len(test_index)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1sQ8UqjGaQm"
      },
      "source": [
        "Now we perform cross-validation and we tell sklearn what metrics we want to evaluate\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoeBkG2vLGvB"
      },
      "source": [
        "metrics = [\"accuracy\",\"precision_weighted\",\"recall_weighted\",\"f1_weighted\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTjiOZEFZIkp"
      },
      "source": [
        "cross_val_result = cross_validate(log_reg, nasdaq_train_val_X, nasdaq_train_val_y, cv=tscv.split(nasdaq_train_val_X, nasdaq_train_val_y), scoring=metrics, n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t58fOJI4RXTp"
      },
      "source": [
        "Now let's compute the avarege value for the metrics across the 5 groups:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1fQwp1LQyCb"
      },
      "source": [
        "for i in cross_val_result:\n",
        "    cross_val_result[i] = cross_val_result[i].mean()\n",
        "cross_val_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmJeRAM7QxrQ"
      },
      "source": [
        "We see that the result is not that good.\n",
        "\n",
        "This is because this type of cross-validation is not as effective as stratified K-fold validation and because logistic regression may not be the best model for our task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zOqEhrDUxlo"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2HC8MiAUhZl"
      },
      "source": [
        "Let's try evaluating the model on the validation set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ueeoi8IJps9J"
      },
      "source": [
        "pred_val = log_reg.predict(nasdaq_val_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrdABfgqRrai"
      },
      "source": [
        "print(classification_report(nasdaq_val_y, pred_val, digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9IeULvTU6kb"
      },
      "source": [
        "The lines of the ouput we are interested in are the one about accuracy and the last one containing the weighted metrics.\n",
        "\n",
        "As we can see the result of this validation is better than the one we had for cross-validation. This is because cross-validation is not highly reliable for time series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhB-0NZQVqrZ"
      },
      "source": [
        "Let's also look at the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6heQcew7DCQS"
      },
      "source": [
        "confusion_matrix(nasdaq_val_y, pred_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-HjOfe7V3P_"
      },
      "source": [
        "And finally we plot also the ROC curve for our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzRk_NT6cNA2"
      },
      "source": [
        "To do this we calculate the classification scores for the instances of the validation set.\n",
        "\n",
        "Note that we cannot use `cross_val_predict()` as it is not compatible with TimeSeries split for obvious reasons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0g0SWvJbwLg"
      },
      "source": [
        "pred_val_scores = log_reg.decision_function(nasdaq_val_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw6XpAWFjB1Q"
      },
      "source": [
        "fpr_log_reg, tpr_log_reg, thresholds = roc_curve(nasdaq_val_y, pred_val_scores)\n",
        "\n",
        "def plot_roc_curve(fpr, tpr, labels=None):\n",
        "    for fpr_i, tpr_i, label_i in zip(fpr, tpr, labels):\n",
        "        plt.plot(fpr_i, tpr_i, linewidth=2, label=label_i)\n",
        "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random classifier\")\n",
        "    plt.axis([0, 1, 0, 1.01])\n",
        "    plt.xlabel(\"False positive rate (fpr)\")\n",
        "    plt.ylabel(\"True positive rate (tpr)\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    \n",
        "plot_roc_curve([fpr_log_reg], [tpr_log_reg], labels=[\"Logistic regression\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVna95aZWPk5"
      },
      "source": [
        "Looking at the area underneath the curve we can see how well our model performs compared to the random classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUI-ghrRXSaZ"
      },
      "source": [
        "roc_auc_score(nasdaq_val_y, pred_val_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycc4rSVXXeSk"
      },
      "source": [
        "The more the area is grater than 0.5, the more our model performs better than random."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lujSh-xXX8pD"
      },
      "source": [
        "### Considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0OxT4SjYBQW"
      },
      "source": [
        "This model achieves a Weighted-$F_1$-Score of 0.5723 on the validation set.\n",
        "\n",
        "The authors of the paper run their algorithm multiple times for comparison porpuses and obtain, for 2D-CNNPred, a Macro-Averaged-$F_{1}$-Measure mean of 0.4779, with a maximum value of 0.5219 on the NASDAQ dataset.\n",
        "\n",
        "However their runs refer to the test set, for which we still don't know how this model will preform.\n",
        "\n",
        "It is also to be kept in mind that 2D-CNNPred is trained using all the datasets, while our model considers only NASDAQ Composite.\n",
        "\n",
        "We will not yet look at the performance of our models on the test set.\n",
        "This wil be done only at the very end of the project after we select the best performing model based on the validation performances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c1dSjR2RVbM"
      },
      "source": [
        "val_performance_acc['Log Reg'] = accuracy_score(nasdaq_val_y, pred_val)\n",
        "val_performance_f1_weighted['Log Reg'] = f1_score(nasdaq_val_y, pred_val, average='weighted')\n",
        "\n",
        "pred_test =  log_reg.predict(nasdaq_test_X)\n",
        "\n",
        "test_performance_acc['Log Reg'] = accuracy_score(nasdaq_test_y, pred_test)\n",
        "test_performance_f1_weighted['Log Reg'] = f1_score(nasdaq_test_y, pred_test, average='weighted')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ncliy-Bu8-K"
      },
      "source": [
        "## Ensemble hard voting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QvS913_9TGO"
      },
      "source": [
        "Now we try to use an ensemble of 4 classifiers, the used classifiers are:\n",
        "\n",
        "*   Gaussian Naive Bayes (we infer the prior of the classes directly from the data)\n",
        "*   Logistic Regression (the one that we just used)\n",
        "*   Perceptron\n",
        "*   Support Vector Machine\n",
        "\n",
        "We use hard voting as the SGD classifier used to model the perceptron does not provide probability estimates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3CpTl3mvEj1"
      },
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "gnb_clf = GaussianNB()\n",
        "log_clf = SGDClassifier(max_iter=400, tol=None, random_state=42,\n",
        "                        loss=\"log\", eta0=0.4, learning_rate=\"adaptive\", penalty=None, n_jobs=-1)\n",
        "perc_clf = SGDClassifier(max_iter=400, tol=None, random_state=42,\n",
        "                   loss=\"perceptron\", eta0=1, learning_rate=\"constant\", penalty=None, n_jobs=-1)\n",
        "svm_clf = SVC(random_state=42, probability=True)\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('gnb', gnb_clf), ('log', log_clf), ('perc', perc_clf), ('svm',svm_clf)],\n",
        "    voting='hard')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLcqgo6qBbir"
      },
      "source": [
        "Let's now see the evaluation metrics on each single classifier and on the voting classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80_EMR-AvRj_"
      },
      "source": [
        "for clf in (gnb_clf, log_clf, perc_clf, svm_clf, voting_clf):\n",
        "    clf.fit(nasdaq_train_X, nasdaq_train_y)\n",
        "    pred_val = clf.predict(nasdaq_val_X)\n",
        "    print(clf.__class__.__name__, '\\n', classification_report(nasdaq_val_y, pred_val, digits=3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVWsQkQHABnm"
      },
      "source": [
        "We can see that singularly SVM and Naive Bayes do nat perform very good, but in the ensemble with the Logistic Regresssion and the Perceptron (which we see performing slightly better) they provide surprising results.\n",
        "\n",
        "Although it is hard to train the models in a way that they provide uncorrelated errors, the result we obtain with this method is an improvement with respect to the Logistic Regression classifier (treated in the previous section) alone.\n",
        "\n",
        "The voting classifier achieves perfect results on the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I05uCRbABnw"
      },
      "source": [
        "pred_train = voting_clf.predict(nasdaq_train_X)\n",
        "\n",
        "confusion_matrix(nasdaq_train_y, pred_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS72ttTtAWft"
      },
      "source": [
        "### Cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcHXzXcTAWft"
      },
      "source": [
        "Now let's evaluate how well it generalises performing cross-validation (details on this have been given in [this section](#cross_val))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGm-Qh0GAWfv"
      },
      "source": [
        "cross_val_result = cross_validate(voting_clf, nasdaq_train_val_X, nasdaq_train_val_y, cv=tscv.split(nasdaq_train_val_X, nasdaq_train_val_y), scoring=metrics, n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy-MJhLSAWfv"
      },
      "source": [
        "Now let's compute the avarege value for the metrics across the 5 groups:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG8VmuFtAWfv"
      },
      "source": [
        "for i in cross_val_result:\n",
        "    cross_val_result[i] = cross_val_result[i].mean()\n",
        "cross_val_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdFfPEezEXqP"
      },
      "source": [
        "We see that the result is not that good.\n",
        "\n",
        "This is because this type of cross-validation is not as effective as stratified K-fold validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m5M3T02DI1g"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPUzWZjsDI1g"
      },
      "source": [
        "Let's try evaluating the model on the validation set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp3IbRmNDI1g"
      },
      "source": [
        "pred_val = voting_clf.predict(nasdaq_val_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlNSkH6FDI1h"
      },
      "source": [
        "print(classification_report(nasdaq_val_y, pred_val, digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCdt7hLfDI1h"
      },
      "source": [
        "The lines of the ouput we are interested in are the one about accuracy and the last one containing the weighted metrics.\n",
        "\n",
        "As we can see the result of this validation is better than the one we had for cross-validation. This is because cross-validation is not highly reliable for time series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiSQYhWyDI1j"
      },
      "source": [
        "Let's also look at the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIRaN2BnDI1j"
      },
      "source": [
        "confusion_matrix(nasdaq_val_y, pred_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyIVHb8rDI1j"
      },
      "source": [
        "We cannot loo at the ROC Curve as the classifier uses hard voting and therefore doesn't output the probability of the predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTsz_HFrDe4c"
      },
      "source": [
        "### Considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnGlHIY7De4d"
      },
      "source": [
        "This model achieves a Weighted-$F_1$-Score of 0.592 on the validation set, which is better than the result achieved by Logistic Regression alone.\n",
        "\n",
        "We will not yet look at the performance of our models on the test set.\n",
        "This wil be done only at the very end of the project after we select the best performing model based on the validation performances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlHAs2sZRiTJ"
      },
      "source": [
        "val_performance_acc['Voting Clf'] = accuracy_score(nasdaq_val_y, pred_val)\n",
        "val_performance_f1_weighted['Voting Clf'] = f1_score(nasdaq_val_y, pred_val, average='weighted')\n",
        "\n",
        "pred_test =  voting_clf.predict(nasdaq_test_X)\n",
        "\n",
        "test_performance_acc['Voting Clf'] = accuracy_score(nasdaq_test_y, pred_test)\n",
        "test_performance_f1_weighted['Voting Clf'] = f1_score(nasdaq_test_y, pred_test, average='weighted')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq8tmvG_pXZm"
      },
      "source": [
        "## Neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwFPzpLQonmc"
      },
      "source": [
        "This is the function we will use to compule and fit our neuarl networks.\n",
        "\n",
        "It uses early stopping, which means that the training will stop as soon as the validation loss starts to increase. The patience for early stopping is chosen by the user.\n",
        "\n",
        "The optimizer used is Adam which has been chosen after accurate review of available optimizers.\n",
        "\n",
        "The function gives to the user the possibility to add a dimension to the data (that is used for 3D CNNs).\n",
        "\n",
        "Finally the evaluation metric is the accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDj3zRMHpeNt"
      },
      "source": [
        "MAX_EPOCHS = 20\n",
        "\n",
        "def compile_and_fit(model, patience=2, add_dim=False, use_sparse=False, batch_size=32, optimizer=tf.keras.optimizers.Adam()):\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    if add_dim:\n",
        "        train_set = nasdaq_train_X_3D[:,:,:,np.newaxis]\n",
        "        val_set = nasdaq_val_X_3D[:,:,:,np.newaxis]\n",
        "    else:\n",
        "        train_set = nasdaq_train_X_3D\n",
        "        val_set = nasdaq_val_X_3D\n",
        "\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                    patience=patience,\n",
        "                                                    mode='auto')\n",
        "\n",
        "\n",
        "    loss = tf.losses.SparseCategoricalCrossentropy() if use_sparse else tf.losses.BinaryCrossentropy()\n",
        "\n",
        "    model.compile(loss=loss,\n",
        "                optimizer=optimizer,\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(train_set, nasdaq_train_y, \n",
        "                        epochs=MAX_EPOCHS,\n",
        "                        validation_data=(val_set, nasdaq_val_y),\n",
        "                        callbacks=[early_stopping],\n",
        "                        batch_size=batch_size)\n",
        "        \n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUrlca_CpCP5"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOV08V4qql5e"
      },
      "source": [
        "Long-Short-Term memory cells are able to learn what to remember and what to forget from previously seen data.\n",
        "\n",
        "For this reason it seems reasonable to apply them to time series prediction. As they will be able to model the correlations in the time series and predict the next step.\n",
        "\n",
        "The LSTM layer is fed a $n*83$ vector, with $n$ the number of days.\n",
        "\n",
        "It analyses incrementally day by day and provides one output.\n",
        "\n",
        "In our scenario we want to output just the prediction at the last time step so we set `return_sequences` to `False`.\n",
        "\n",
        "The output in this case is a vector containing the probabilities of classes 0 and 1. Therefore the loss function used will be `SparseCategoricalCrossentropy`.\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69sHv6S-2nC0"
      },
      "source": [
        "### Hyperparams search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO0bJHRnKFpa"
      },
      "source": [
        "For this model we perform hyperparameter grid search using the *hparams* plugin of tensorboard.\n",
        "\n",
        "A first pass of hyperparameters selection has shown that SGD was the best optimizer for this model.\n",
        "\n",
        "In the second pass, reported in this section, we are investigating the number of LSTM units in the LSTM layer and the learning rate for SGD.\n",
        "\n",
        "The chosen interval to investigate for the learning rate is (0.01, 0.1) as values in this range have been foung to provide the highest performance.\n",
        "\n",
        "First we set the hyperparameters in which we are interested and their domains:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqbhxgIG2mwU"
      },
      "source": [
        "!rm -rf ./logs\n",
        "\n",
        "HP_NUM_UNITS_LSTM = hp.HParam('num_units_lstm', hp.IntInterval(1,200))\n",
        "HP_LEARNING_RATE = hp.HParam('alpha', hp.RealInterval(0.01, 0.1))\n",
        "\n",
        "METRIC_ACCURACY = 'accuracy'\n",
        "\n",
        "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
        "  hp.hparams_config(\n",
        "    hparams=[HP_NUM_UNITS_LSTM, HP_LEARNING_RATE],\n",
        "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGM9kZirLfOU"
      },
      "source": [
        "Then we define a function for training the model that is parametric in the hyperparameters and interacts with tensorboard through two callbacks.\n",
        "\n",
        "This function is very similar to the genear puprpose `compile_and_fit` function defined above, but contains the network layers definition inside andalso performs evaluation outputting the accuracy achieved on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YQ7FIrT212m"
      },
      "source": [
        "def auto_compile_and_fit(hparams, hparam_dir, patience=2, add_dim=False, use_sparse=True, batch_size=32):\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    lstm_model = tf.keras.models.Sequential([                                  \n",
        "        tf.keras.layers.LSTM(hparams[HP_NUM_UNITS_LSTM], return_sequences=False),\n",
        "        tf.keras.layers.Dense(units=2, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=hparam_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                patience=patience,\n",
        "                                                mode='auto')\n",
        "\n",
        "    loss = tf.losses.SparseCategoricalCrossentropy() if use_sparse else tf.losses.BinaryCrossentropy()\n",
        "\n",
        "    lstm_model.compile(loss=loss,\n",
        "                optimizer=tf.keras.optimizers.SGD(learning_rate=hparams[HP_LEARNING_RATE]),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    history = lstm_model.fit(nasdaq_train_X_3D, nasdaq_train_y, \n",
        "                        epochs=MAX_EPOCHS,\n",
        "                        validation_data=(nasdaq_val_X_3D, nasdaq_val_y),\n",
        "                        callbacks=[tensorboard_callback, # log metrics\n",
        "                                    hp.KerasCallback(hparam_dir, hparams),\n",
        "                                    early_stopping],\n",
        "                        batch_size=batch_size)\n",
        "    \n",
        "    _, accuracy = lstm_model.evaluate(nasdaq_val_X_3D, nasdaq_val_y)\n",
        "        \n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wolKIusLNKo1"
      },
      "source": [
        "Then, for each hyperparameter that we are interested in, we perform the grid search. Writing the resulting accuracy in the logs file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVIY2tda5lvL"
      },
      "source": [
        "session_num = 0\n",
        "\n",
        "for learning_rate in np.arange(HP_LEARNING_RATE.domain.min_value, HP_LEARNING_RATE.domain.max_value, 0.02):\n",
        "    for num_units_lstm in np.arange(HP_NUM_UNITS_LSTM.domain.min_value, HP_NUM_UNITS_LSTM.domain.max_value, 30):\n",
        "        hparams = {\n",
        "            HP_LEARNING_RATE: learning_rate,\n",
        "            HP_NUM_UNITS_LSTM: num_units_lstm\n",
        "        }\n",
        "        run_name = \"run-%d\" % session_num\n",
        "        print('--- Starting trial: %s' % run_name)\n",
        "        print({h.name: hparams[h] for h in hparams})\n",
        "        run_dir = 'logs/hparam_tuning/' + run_name\n",
        "\n",
        "        with tf.summary.create_file_writer(run_dir).as_default():\n",
        "            accuracy = auto_compile_and_fit(hparams, hparam_dir=run_dir)\n",
        "            tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
        "        session_num += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhi9NAedT73v"
      },
      "source": [
        "After the search is finished, we are able to visualize in tensorboard the hyperparameters that provide the best results.\n",
        "\n",
        "In our case these are:\n",
        "\n",
        "*   61 LSTM cells\n",
        "*   0.08 as learning rate for SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OljyuygG60C5"
      },
      "source": [
        "%tensorboard --logdir logs/hparam_tuning/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz8leadpDjwO"
      },
      "source": [
        "### Selected model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qszr6bG4UZpI"
      },
      "source": [
        "Now we train the model with the chosen hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3DFsyZapFJm"
      },
      "source": [
        "lstm_model = tf.keras.models.Sequential([                                  \n",
        "    # Shape [batch, days, features] => [batch, lstm_units]\n",
        "    tf.keras.layers.LSTM(61, return_sequences=False),\n",
        "    tf.keras.layers.Dense(units=2, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biXKJoQ5EWoS"
      },
      "source": [
        "history = compile_and_fit(lstm_model, use_sparse=True, optimizer=tf.keras.optimizers.SGD(learning_rate=0.08))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpqJjP9tUsoj"
      },
      "source": [
        "As the dataset is not very big, the performance of the model depends a lot on the random initialisation of the weights.\n",
        "\n",
        "Therefore to have a stable model I will load the one that achieved the best results.\n",
        "\n",
        "If you want to use the model that we just trained just comment the next cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd-nfNSHVer6"
      },
      "source": [
        "lstm_model = keras.models.load_model('/content/drive/My Drive/Stock_market/Models/LSTM')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzFH8q0BsZF4"
      },
      "source": [
        "Let's now evaluate this model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5qPAqpfApdb"
      },
      "source": [
        "score = lstm_model.evaluate(nasdaq_val_X_3D, nasdaq_val_y)\n",
        "score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LJWD_baVqzw"
      },
      "source": [
        "Here are the metrics on the validation set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Iw_VYJCjm-P"
      },
      "source": [
        "pred_val = tf.argmax(lstm_model.predict(nasdaq_val_X_3D), axis=1)\n",
        "print(classification_report(nasdaq_val_y, pred_val, digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yL6QIP3WRr1"
      },
      "source": [
        "The confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRP4pDgXWO-M"
      },
      "source": [
        "confusion_matrix(nasdaq_val_y, pred_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypPdsfg7Yksa"
      },
      "source": [
        "The ROC curve:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuhHcJDsWeJ-"
      },
      "source": [
        "pred_val_probs = np.max(lstm_model.predict(nasdaq_val_X_3D), axis=1)\n",
        "fpr_lstm, tpr_lstm, thresholds = roc_curve(nasdaq_val_y, pred_val_probs)\n",
        "\n",
        "    \n",
        "plot_roc_curve([fpr_lstm, fpr_log_reg], [tpr_lstm, tpr_log_reg], labels=[\"LSTM\", \"Logistic regression\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI_AMaNOYvsT"
      },
      "source": [
        "Looking at the area underneath the curve we can see how well our model performs compared to the random classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlc-RXsBYvsb"
      },
      "source": [
        "roc_auc_score(nasdaq_val_y, pred_val_probs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF-pDxiwV0Ah"
      },
      "source": [
        "We report also the graphical representation of the network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoB3pFTL7vwF"
      },
      "source": [
        "tf.keras.utils.plot_model(lstm_model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E1qHucGV9C7"
      },
      "source": [
        "And the plot of the training and validation loss and accuracy (this is the history of the model just trained, as the history of the loaded model is not available):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BprJB7B3C0i0"
      },
      "source": [
        "def plot_history(history):\n",
        "    pd.DataFrame(history.history).plot()\n",
        "    plt.grid(True)\n",
        "    plt.gca().set_ylim(0, 1)\n",
        "    plt.gca().set_xlabel(\"Epoch\")\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL5CTnvYY_sT"
      },
      "source": [
        "Finally, we add its performance to the dictionary that contains all the performances of our models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0s9WCe9LSuA"
      },
      "source": [
        "val_performance_acc['LSTM'] = accuracy_score(nasdaq_val_y, pred_val)\n",
        "val_performance_f1_weighted['LSTM'] = f1_score(nasdaq_val_y, pred_val, average='weighted')\n",
        "\n",
        "pred_test = tf.argmax(lstm_model.predict(nasdaq_test_X_3D), axis=1)\n",
        "\n",
        "test_performance_acc['LSTM'] = accuracy_score(nasdaq_test_y, pred_test)\n",
        "test_performance_f1_weighted['LSTM'] = f1_score(nasdaq_test_y, pred_test, average='weighted')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwmsFNftVUmS"
      },
      "source": [
        "## 2D-CNNPred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFjX4Qg5tmM6"
      },
      "source": [
        "In this section we try to recreate the 2D-CNNPred model from the paepr.\n",
        "\n",
        "Given the great amount of information that is missing in the paper, siome parameters have been chosen by me.\n",
        "\n",
        "To measure the loss we use `BinaryCrossEntropy()` and we use early stopping on the validation loss with a patience of 4.\n",
        "\n",
        "Here is the architecture of the network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9JyOy1bU-o0"
      },
      "source": [
        "cnn2d_pred_model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(filters=8, kernel_size=(1,83), input_shape=[60, 83, 1], activation='relu'),\n",
        "    keras.layers.Dropout(rate=0.1),\n",
        "    keras.layers.Conv2D(filters=8, kernel_size=(3,1), activation='relu'),\n",
        "    keras.layers.Dropout(rate=0.1),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2,1)),\n",
        "    keras.layers.Conv2D(filters=8, kernel_size=(3,1), activation='relu'),\n",
        "    keras.layers.Dropout(rate=0.1),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2,1)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(units=1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_LMFTyVujfm"
      },
      "source": [
        "Let's train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi3gfCHkcQ3e"
      },
      "source": [
        "history = compile_and_fit(cnn2d_pred_model, add_dim=True, batch_size=128, patience=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KifB6P4wVaD"
      },
      "source": [
        "Now we evaluate the model on the validation set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fglqPTNM_Mq"
      },
      "source": [
        "score = cnn2d_pred_model.evaluate(nasdaq_val_X_3D[:,:,:,np.newaxis], nasdaq_val_y)\n",
        "score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgCcWNMtIOkS"
      },
      "source": [
        "pred_val = np.where(cnn2d_pred_model.predict(nasdaq_val_X_3D[:,:,:,np.newaxis]) > 0.5, 1, 0)\n",
        "print(classification_report(nasdaq_val_y, pred_val, digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCbGRDdbw45p"
      },
      "source": [
        "The confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7GvQLUew45q"
      },
      "source": [
        "confusion_matrix(nasdaq_val_y, pred_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYMavyWGxEpp"
      },
      "source": [
        "The ROC curve:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t120etolxEpq"
      },
      "source": [
        "pred_val_probs = cnn2d_pred_model.predict(nasdaq_val_X_3D[:,:,:,np.newaxis])\n",
        "fpr_2d_cnnpred, tpr_2d_cnnpred, thresholds = roc_curve(nasdaq_val_y, pred_val_probs)\n",
        "\n",
        "    \n",
        "plot_roc_curve([fpr_lstm, fpr_log_reg, fpr_2d_cnnpred], [tpr_lstm, tpr_log_reg, tpr_2d_cnnpred], labels=[\"LSTM\", \"Logistic regression\",\"2D-CNNPred\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H2iJMGkxrCB"
      },
      "source": [
        "Looking at the area underneath the curve we can see how well our model performs compared to the random classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKvYQAPkxrCC"
      },
      "source": [
        "roc_auc_score(nasdaq_val_y, pred_val_probs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gD-6mNIwc0Q"
      },
      "source": [
        "It seems to be way worse compared to the other models, but remember, we still haven't looked at the test set.\n",
        "\n",
        "Let's see a graphical visualization of the network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvPMTQxVdCSS"
      },
      "source": [
        "tf.keras.utils.plot_model(cnn2d_pred_model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9otqhbLiwvxF"
      },
      "source": [
        "And a plot of the loss and accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FvISY6jEgiO"
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYU1-D0bwzCP"
      },
      "source": [
        "Finally, we add its performance to the dictionary that contains all the performances of our models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwYuNzIGLxr6"
      },
      "source": [
        "val_performance_acc['2D-CNNPred'] = accuracy_score(nasdaq_val_y, pred_val)\n",
        "val_performance_f1_weighted['2D-CNNPred'] = f1_score(nasdaq_val_y, pred_val, average='weighted')\n",
        "\n",
        "pred_test =  np.where(cnn2d_pred_model.predict(nasdaq_test_X_3D[:,:,:,np.newaxis]) > 0.5, 1, 0)\n",
        "\n",
        "test_performance_acc['2D-CNNPred'] = accuracy_score(nasdaq_test_y, pred_test)\n",
        "test_performance_f1_weighted['2D-CNNPred'] = f1_score(nasdaq_test_y, pred_test, average='weighted')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRhMWj2YpGp2"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x2RILG5rCWW"
      },
      "source": [
        "### Hyperparameters search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvgYiWWWrRVM"
      },
      "source": [
        "!rm -rf ./logs\n",
        "\n",
        "HP_NUM_UNITS_HIDDEN1 = hp.HParam('num_units_hidden1', hp.IntInterval(1,200))\n",
        "HP_NUM_UNITS_HIDDEN2 = hp.HParam('num_units_hidden2', hp.IntInterval(1,200))\n",
        "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0., 0.8))\n",
        "\n",
        "METRIC_ACCURACY = 'accuracy'\n",
        "\n",
        "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
        "  hp.hparams_config(\n",
        "    hparams=[HP_NUM_UNITS_HIDDEN1, HP_NUM_UNITS_HIDDEN2, HP_DROPOUT],\n",
        "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7_ZeDJ2rRVM"
      },
      "source": [
        "Then we define a function for training the model that is parametric in the hyperparameters and interacts with tensorboard through two callbacks.\n",
        "\n",
        "This function is very similar to the genear puprpose `compile_and_fit` function defined above, but contains the network layers definition inside andalso performs evaluation outputting the accuracy achieved on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ777Fw8rRVM"
      },
      "source": [
        "def auto_compile_and_fit(hparams, hparam_dir, patience=2, add_dim=False, use_sparse=True, batch_size=32):\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    mlp_model = keras.models.Sequential([\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(hparams[HP_NUM_UNITS_HIDDEN1], activation='tanh'),\n",
        "        tf.keras.layers.Dropout(rate=hparams[HP_DROPOUT]),\n",
        "        keras.layers.Dense(hparams[HP_NUM_UNITS_HIDDEN2], activation='tanh'),\n",
        "        tf.keras.layers.Dropout(rate=hparams[HP_DROPOUT]),\n",
        "        keras.layers.Dense(units=2, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=hparam_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                patience=patience,\n",
        "                                                mode='auto')\n",
        "\n",
        "    loss = tf.losses.SparseCategoricalCrossentropy() if use_sparse else tf.losses.BinaryCrossentropy()\n",
        "\n",
        "    mlp_model.compile(loss=loss,\n",
        "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.8),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    history = mlp_model.fit(nasdaq_train_X_3D, nasdaq_train_y, \n",
        "                        epochs=MAX_EPOCHS,\n",
        "                        validation_data=(nasdaq_val_X_3D, nasdaq_val_y),\n",
        "                        callbacks=[tensorboard_callback, # log metrics\n",
        "                                    hp.KerasCallback(hparam_dir, hparams),\n",
        "                                    early_stopping],\n",
        "                        batch_size=batch_size)\n",
        "    \n",
        "    _, accuracy = mlp_model.evaluate(nasdaq_val_X_3D, nasdaq_val_y)\n",
        "        \n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0TxUKTfrRVM"
      },
      "source": [
        "Then, for each hyperparameter that we are interested in, we perform the grid search. Writing the resulting accuracy in the logs file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBhgM7aOrRVN"
      },
      "source": [
        "session_num = 0\n",
        "\n",
        "for num_units_hidden1 in  np.arange(HP_NUM_UNITS_HIDDEN1.domain.min_value, HP_NUM_UNITS_HIDDEN1.domain.max_value, 30):\n",
        "    for num_units_hidden2 in np.arange(HP_NUM_UNITS_HIDDEN2.domain.min_value, HP_NUM_UNITS_HIDDEN2.domain.max_value, 30):\n",
        "        for dropout in np.arange(HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value, 0.4):\n",
        "            hparams = {\n",
        "                HP_NUM_UNITS_HIDDEN1: num_units_hidden1,\n",
        "                HP_NUM_UNITS_HIDDEN2: num_units_hidden2,\n",
        "                HP_DROPOUT: dropout\n",
        "            }\n",
        "            run_name = \"run-%d\" % session_num\n",
        "            print('--- Starting trial: %s' % run_name)\n",
        "            print({h.name: hparams[h] for h in hparams})\n",
        "            run_dir = 'logs/hparam_tuning/' + run_name\n",
        "\n",
        "            with tf.summary.create_file_writer(run_dir).as_default():\n",
        "                accuracy = auto_compile_and_fit(hparams, hparam_dir=run_dir)\n",
        "                tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
        "            session_num += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2EXzAkIuyPd"
      },
      "source": [
        "!kill 3813"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRYjcQKQrRVN"
      },
      "source": [
        "%tensorboard --logdir logs/hparam_tuning/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZK_MPV9rGSw"
      },
      "source": [
        "### Selected model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErSPAd7Zfw0C"
      },
      "source": [
        "mlp_model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(units=181, activation='tanh'),\n",
        "    tf.keras.layers.Dropout(rate=0.4),\n",
        "    keras.layers.Dense(units=61, activation='tanh'),\n",
        "    tf.keras.layers.Dropout(rate=0.4),\n",
        "    keras.layers.Dense(units=2, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jln-ytFAgUy5"
      },
      "source": [
        "history = compile_and_fit(mlp_model, use_sparse=True, optimizer=keras.optimizers.SGD(learning_rate=0.08))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7aE0OVxvjiC"
      },
      "source": [
        "mlp_model = tf.keras.models.load_model('/content/drive/My Drive/Stock_market/Models/MLP')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCj0207tgzRY"
      },
      "source": [
        "pred_val = tf.argmax(mlp_model.predict(nasdaq_val_X_3D), axis=1)\n",
        "print(classification_report(nasdaq_val_y, pred_val, digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkUl7Zj3hWv2"
      },
      "source": [
        "val_performance_acc['MLP'] = accuracy_score(nasdaq_val_y, pred_val)\n",
        "val_performance_f1_weighted['MLP'] = f1_score(nasdaq_val_y, pred_val, average='weighted')\n",
        "\n",
        "pred_test = tf.argmax(mlp_model.predict(nasdaq_test_X_3D), axis=1)\n",
        "\n",
        "test_performance_acc['MLP'] = accuracy_score(nasdaq_test_y, pred_test)\n",
        "test_performance_f1_weighted['MLP'] = f1_score(nasdaq_test_y, pred_test, average='weighted')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BblRCBHv5skE"
      },
      "source": [
        "## Other classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dThGfs_5yVh"
      },
      "source": [
        "We have also tried a selection of various classifiers which have been less succesfull.\n",
        "\n",
        "Here we briefly report them:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0pptvFu6BZV"
      },
      "source": [
        "### Bagging classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2cgTFhS_QX7"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "\n",
        "#use bootstrap=False for pasting\n",
        "bag_clf = BaggingClassifier(\n",
        "    SGDClassifier(max_iter=300, tol=None, random_state=42,\n",
        "                        loss=\"log\", eta0=0.4, learning_rate=\"adaptive\", penalty=None, n_jobs=-1),\n",
        "    n_estimators=200,\n",
        "    max_samples=nasdaq_train_X.shape[0], max_features=0.8, \n",
        "    bootstrap=True, bootstrap_features=False,\n",
        "    n_jobs=-1, random_state=42)\n",
        "\n",
        "bag_clf.fit(nasdaq_train_X, nasdaq_train_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV3r7iH5_onj"
      },
      "source": [
        "pred_val = bag_clf.predict(nasdaq_val_X)\n",
        "print(classification_report(nasdaq_val_y, pred_val, digits=3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygWgSZl0S4Pk"
      },
      "source": [
        "### Random forrest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLqnlDuSS89n"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators=300, max_leaf_nodes=40, \n",
        "                                 n_jobs=-1, random_state=42)\n",
        "\n",
        "rnd_clf.fit(nasdaq_train_X, nasdaq_train_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qGEt94_YAyg"
      },
      "source": [
        "pred_val = rnd_clf.predict(nasdaq_val_X)\n",
        "print(classification_report(nasdaq_val_y, pred_val, digits=3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck6lsSaSeyXe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrLxoZfcQ8br"
      },
      "source": [
        "## Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4C-G_YTUKAW"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIbIQEbZMCfs"
      },
      "source": [
        "x = np.arange(len(val_performance_acc))\n",
        "width = 0.2\n",
        "val_acc = list(val_performance_acc.values())\n",
        "\n",
        "\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Model')\n",
        "plt.bar(x, val_acc, width, label='Validation')\n",
        "\n",
        "plt.xticks(ticks=x, labels=val_performance_acc.keys(),\n",
        "           rotation=45)\n",
        "plt.ylim(0, np.max(val_acc) + 0.05)\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oopXewK3QLrr"
      },
      "source": [
        "x = np.arange(len(val_performance_f1_weighted))\n",
        "width = 0.2\n",
        "val_f1 = list(val_performance_f1_weighted.values())\n",
        "\n",
        "\n",
        "plt.ylabel('$F_1$ Weighted')\n",
        "plt.xlabel('Model')\n",
        "plt.bar(x, val_f1, width, label='Validation')\n",
        "\n",
        "plt.xticks(ticks=x, labels=val_performance_acc.keys(),\n",
        "           rotation=45)\n",
        "plt.ylim(0, np.max(val_f1) + 0.05)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpYupdj0UOri"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fC8BWtKUDT9"
      },
      "source": [
        "x = np.arange(len(test_performance_acc))\n",
        "width = 0.2\n",
        "\n",
        "val_acc = list(val_performance_acc.values())\n",
        "test_acc = list(test_performance_acc.values())\n",
        "\n",
        "plt.bar(x - 0.17, val_acc, width, label='Validation')\n",
        "plt.bar(x + 0.17, test_acc, width, label='Test')\n",
        "\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Model')\n",
        "\n",
        "plt.xticks(ticks=x, labels=test_performance_acc.keys(),\n",
        "           rotation=45)\n",
        "plt.ylim(0, np.max([val_acc, test_acc]) + 0.05)\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM1k56i1V-S9"
      },
      "source": [
        "x = np.arange(len(test_performance_f1_weighted))\n",
        "width = 0.2\n",
        "\n",
        "val_f1 = list(val_performance_f1_weighted.values())\n",
        "test_f1 = list(test_performance_f1_weighted.values())\n",
        "\n",
        "plt.bar(x - 0.17, val_f1, width, label='Validation')\n",
        "plt.bar(x + 0.17, test_f1, width, label='Test')\n",
        "\n",
        "plt.ylabel('$F_1$ Weighted')\n",
        "plt.xlabel('Model')\n",
        "\n",
        "plt.xticks(ticks=x, labels=test_performance_f1_weighted.keys(),\n",
        "           rotation=45)\n",
        "plt.ylim(0, np.max([val_f1, test_f1]) + 0.05)\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQr6Qv9c2HO7"
      },
      "source": [
        "# Visualisation and dimensionality reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh9OrTR9yjoN"
      },
      "source": [
        "### PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_1Hl7jNylB5"
      },
      "source": [
        "from sklearn import decomposition\n",
        "\n",
        "X = nasdaq_train_X\n",
        "\n",
        "pca = sklearn.decomposition.PCA()\n",
        "pca_result = pca.fit_transform(X)\n",
        "\n",
        "# Find the predictions from the PCA model, using just the first component\n",
        "print(pca.mean_.shape, pca.components_.shape, pca_result.shape)\n",
        "\n",
        "p1,p2 = pca_result[:,0], pca_result[:,1]\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(14,11))\n",
        "ax.scatter(p1, p2, alpha=.2)\n",
        "\n",
        "for lvl in set(nasdaq_train_y):\n",
        "    i = (nasdaq_train_y == lvl)\n",
        "    plt.scatter(p1[i], p2[i], label=lvl)\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1.1, 1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPC97QnG4Cgl"
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure(1)\n",
        "plt.clf()\n",
        "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
        "\n",
        "p1,p2, p3 = pca_result[:,0], pca_result[:,1], pca_result[:,2]\n",
        "\n",
        "plt.cla()\n",
        "for lvl in set(nasdaq_train_y):\n",
        "    i = (nasdaq_train_y == lvl)\n",
        "    ax.scatter(p1[i], p2[i], p3[i], label=lvl)\n",
        "    ax.legend()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvtf7cuP2L9z"
      },
      "source": [
        "### t-SNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as3KyqUh2Npz"
      },
      "source": [
        "from sklearn import manifold\n",
        "\n",
        "tsne = sklearn.manifold.TSNE(n_components=2, verbose=1)\n",
        "tsne_results = tsne.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyjAqnc02fEy"
      },
      "source": [
        "p1,p2 = tsne_results[:,0], tsne_results[:,1]\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(14,11))\n",
        "ax.scatter(p1, p2, alpha=.2)\n",
        "ax.set_aspect('equal')\n",
        "for lvl in set(nasdaq_train_y):\n",
        "    i = (nasdaq_train_y == lvl)\n",
        "    plt.scatter(p1[i], p2[i], label=lvl)\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1.1, 1))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cvCuRNM2wRM"
      },
      "source": [
        "tsne = sklearn.manifold.TSNE(n_components=2, perplexity=5)\n",
        "tsne_results = tsne.fit_transform(X)\n",
        "\n",
        "p1,p2 = tsne_results[:,0], tsne_results[:,1]\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(14,11))\n",
        "ax.scatter(p1, p2, alpha=.2)\n",
        "ax.set_aspect('equal')\n",
        "for lvl in set(nasdaq_train_y):\n",
        "    i = (nasdaq_train_y == lvl)\n",
        "    plt.scatter(p1[i], p2[i], label=lvl)\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1.1, 1))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7SOITnZ3Py_"
      },
      "source": [
        "tsne = sklearn.manifold.TSNE(n_components=2, perplexity=20)\n",
        "tsne_results = tsne.fit_transform(X)\n",
        "\n",
        "p1,p2 = tsne_results[:,0], tsne_results[:,1]\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(14,11))\n",
        "ax.scatter(p1, p2, alpha=.2)\n",
        "ax.set_aspect('equal')\n",
        "for lvl in set(nasdaq_train_y):\n",
        "    i = (nasdaq_train_y == lvl)\n",
        "    plt.scatter(p1[i], p2[i], label=lvl)\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1.1, 1))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilwsG6ZW3T6W"
      },
      "source": [
        "tsne = sklearn.manifold.TSNE(n_components=2, perplexity=30)\n",
        "tsne_results = tsne.fit_transform(X)\n",
        "\n",
        "p1,p2 = tsne_results[:,0], tsne_results[:,1]\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(14,11))\n",
        "ax.scatter(p1, p2, alpha=.2)\n",
        "ax.set_aspect('equal')\n",
        "for lvl in set(nasdaq_train_y):\n",
        "    i = (nasdaq_train_y == lvl)\n",
        "    plt.scatter(p1[i], p2[i], label=lvl)\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1.1, 1))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMhC4Bdk3VeK"
      },
      "source": [
        "tsne = sklearn.manifold.TSNE(n_components=2, perplexity=40)\n",
        "tsne_results = tsne.fit_transform(X)\n",
        "\n",
        "p1,p2 = tsne_results[:,0], tsne_results[:,1]\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(14,11))\n",
        "ax.scatter(p1, p2, alpha=.2)\n",
        "ax.set_aspect('equal')\n",
        "for lvl in set(nasdaq_train_y):\n",
        "    i = (nasdaq_train_y == lvl)\n",
        "    plt.scatter(p1[i], p2[i], label=lvl)\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1.1, 1))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx6JKrbB3W1W"
      },
      "source": [
        "tsne = sklearn.manifold.TSNE(n_components=2, perplexity=50)\n",
        "tsne_results = tsne.fit_transform(X)\n",
        "\n",
        "p1,p2 = tsne_results[:,0], tsne_results[:,1]\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(14,11))\n",
        "ax.scatter(p1, p2, alpha=.2)\n",
        "ax.set_aspect('equal')\n",
        "for lvl in set(nasdaq_train_y):\n",
        "    i = (nasdaq_train_y == lvl)\n",
        "    plt.scatter(p1[i], p2[i], label=lvl)\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1.1, 1))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMXfkptS3Z_J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}